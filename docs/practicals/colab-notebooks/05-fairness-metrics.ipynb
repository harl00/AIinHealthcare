{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öñÔ∏è Exercise 5: Measuring Algorithmic Fairness\n",
        "\n",
        "**Week 5 | AI in Healthcare Curriculum**\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this exercise, you will:\n",
        "\n",
        "- üéØ Train a clinical prediction model and evaluate its overall performance\n",
        "- üéØ Calculate and interpret performance metrics stratified by demographic groups\n",
        "- üéØ Apply formal fairness metrics (demographic parity, equalised odds)\n",
        "- üéØ Understand the trade-offs between different fairness definitions\n",
        "- üéØ Develop a governance recommendation based on fairness analysis\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è±Ô∏è Estimated Time: 2 hours\n",
        "\n",
        "---\n",
        "\n",
        "## Context\n",
        "\n",
        "In Week 3, you explored a healthcare dataset and identified representation gaps and outcome disparities. Now we'll take the next step: **training a model and measuring whether it performs fairly.**\n",
        "\n",
        "An AI model might achieve excellent *overall* performance while systematically underperforming for certain patient groups. This exercise will help you:\n",
        "- Identify such disparities\n",
        "- Quantify them using formal metrics\n",
        "- Make informed governance decisions\n",
        "\n",
        "**Clinical Scenario:** You're evaluating a deterioration prediction model for possible deployment. Before recommending approval, you need to assess whether it performs equitably across different patient populations."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Setup and Data Preparation"
      ],
      "metadata": {
        "id": "part1-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup - run this first!\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, \n",
        "    roc_auc_score, confusion_matrix, classification_report,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the same ED dataset from Exercise 3\n",
        "\n",
        "def generate_ed_dataset(n_patients=2000):\n",
        "    \"\"\"\n",
        "    Generate a synthetic ED dataset with realistic patterns,\n",
        "    including intentional biases for educational purposes.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Demographics\n",
        "    ages = np.random.normal(55, 20, n_patients).clip(18, 95).astype(int)\n",
        "    genders = np.random.choice(['Male', 'Female'], n_patients, p=[0.52, 0.48])\n",
        "    \n",
        "    # Indigenous status - intentionally underrepresented\n",
        "    indigenous_status = np.random.choice(\n",
        "        ['Non-Indigenous', 'Aboriginal', 'Torres Strait Islander', 'Both', 'Not Stated'],\n",
        "        n_patients,\n",
        "        p=[0.85, 0.03, 0.01, 0.005, 0.105]\n",
        "    )\n",
        "    \n",
        "    # Remoteness - urban overrepresentation\n",
        "    remoteness = np.random.choice(\n",
        "        ['Major City', 'Inner Regional', 'Outer Regional', 'Remote', 'Very Remote'],\n",
        "        n_patients,\n",
        "        p=[0.75, 0.15, 0.07, 0.02, 0.01]\n",
        "    )\n",
        "    \n",
        "    # Socioeconomic status (SEIFA-like decile)\n",
        "    seifa_decile = np.random.choice(range(1, 11), n_patients, \n",
        "                                     p=[0.05, 0.06, 0.07, 0.08, 0.09, 0.11, 0.12, 0.14, 0.14, 0.14])\n",
        "    \n",
        "    # Clinical data\n",
        "    triage_category = np.random.choice([1, 2, 3, 4, 5], n_patients,\n",
        "                                        p=[0.03, 0.12, 0.35, 0.40, 0.10])\n",
        "    \n",
        "    # Vital signs\n",
        "    heart_rate = np.random.normal(85, 18, n_patients).clip(40, 180)\n",
        "    respiratory_rate = np.random.normal(18, 5, n_patients).clip(8, 40)\n",
        "    systolic_bp = np.random.normal(125, 22, n_patients).clip(70, 200)\n",
        "    temperature = np.random.normal(37.0, 0.7, n_patients).clip(35, 41)\n",
        "    oxygen_saturation = np.random.normal(96, 3, n_patients).clip(80, 100)\n",
        "    \n",
        "    # Comorbidities\n",
        "    comorbidity_count = np.random.poisson(1.5, n_patients).clip(0, 8)\n",
        "    \n",
        "    # Outcomes - with bias related to socioeconomic status and remoteness\n",
        "    base_risk = (\n",
        "        0.01 * (ages - 50) / 10 +\n",
        "        0.02 * (5 - triage_category) +\n",
        "        0.01 * comorbidity_count +\n",
        "        0.005 * (heart_rate - 80) / 20 +\n",
        "        -0.005 * (seifa_decile - 5) +\n",
        "        0.03 * np.isin(remoteness, ['Remote', 'Very Remote']).astype(int) +\n",
        "        np.random.normal(0, 0.03, n_patients)\n",
        "    )\n",
        "    adverse_outcome = (base_risk > 0.15).astype(int)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'patient_id': [f'ED{i:05d}' for i in range(n_patients)],\n",
        "        'age': ages,\n",
        "        'gender': genders,\n",
        "        'indigenous_status': indigenous_status,\n",
        "        'remoteness': remoteness,\n",
        "        'seifa_decile': seifa_decile,\n",
        "        'triage_category': triage_category,\n",
        "        'heart_rate': heart_rate.round(0).astype(int),\n",
        "        'respiratory_rate': respiratory_rate.round(0).astype(int),\n",
        "        'systolic_bp': systolic_bp.round(0).astype(int),\n",
        "        'temperature': temperature.round(1),\n",
        "        'oxygen_saturation': oxygen_saturation.round(0).astype(int),\n",
        "        'comorbidity_count': comorbidity_count,\n",
        "        'adverse_outcome': adverse_outcome\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Generate dataset\n",
        "ed_data = generate_ed_dataset(2000)\n",
        "\n",
        "# Create grouped variables for fairness analysis\n",
        "ed_data['indigenous_grouped'] = ed_data['indigenous_status'].apply(\n",
        "    lambda x: 'Indigenous' if x in ['Aboriginal', 'Torres Strait Islander', 'Both'] \n",
        "    else ('Not Stated' if x == 'Not Stated' else 'Non-Indigenous')\n",
        ")\n",
        "\n",
        "ed_data['remoteness_grouped'] = ed_data['remoteness'].apply(\n",
        "    lambda x: 'Remote' if x in ['Remote', 'Very Remote'] else 'Non-Remote'\n",
        ")\n",
        "\n",
        "ed_data['seifa_grouped'] = pd.cut(ed_data['seifa_decile'], \n",
        "                                   bins=[0, 3, 7, 10], \n",
        "                                   labels=['Low (1-3)', 'Medium (4-7)', 'High (8-10)'])\n",
        "\n",
        "print(\"Emergency Department Dataset Generated\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total presentations: {len(ed_data):,}\")\n",
        "print(f\"Adverse outcome rate: {ed_data['adverse_outcome'].mean()*100:.1f}%\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "ed_data.head()"
      ],
      "metadata": {
        "id": "generate-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Train a Clinical Prediction Model\n",
        "\n",
        "We'll train a Random Forest classifier to predict adverse outcomes (deterioration). This simulates the type of model a vendor might offer."
      ],
      "metadata": {
        "id": "part2-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features for the model\n",
        "# Note: We're using only clinical features, not demographics\n",
        "# This is a common approach to avoid \"encoding\" demographics directly\n",
        "\n",
        "feature_columns = [\n",
        "    'age', 'triage_category', 'heart_rate', 'respiratory_rate',\n",
        "    'systolic_bp', 'temperature', 'oxygen_saturation', 'comorbidity_count'\n",
        "]\n",
        "\n",
        "X = ed_data[feature_columns]\n",
        "y = ed_data['adverse_outcome']\n",
        "\n",
        "# Store demographic columns for fairness analysis\n",
        "demographics = ed_data[['patient_id', 'gender', 'indigenous_grouped', \n",
        "                        'remoteness_grouped', 'seifa_grouped']].copy()\n",
        "\n",
        "print(\"Features used for prediction:\")\n",
        "print(\"-\" * 40)\n",
        "for col in feature_columns:\n",
        "    print(f\"  ‚Ä¢ {col}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Note: Demographic variables are NOT used as model inputs\")\n",
        "print(f\"   However, this doesn't guarantee fair outcomes!\")"
      ],
      "metadata": {
        "id": "prepare-features"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Split demographics to match\n",
        "demographics_test = demographics.iloc[X_test.index].copy()\n",
        "\n",
        "print(f\"Training set: {len(X_train)} patients\")\n",
        "print(f\"Test set: {len(X_test)} patients\")\n",
        "print(f\"\\nOutcome distribution in test set: {y_test.mean()*100:.1f}% adverse\")"
      ],
      "metadata": {
        "id": "split-data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Random Forest model\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'  # Helps with imbalanced outcomes\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úÖ Model trained successfully!\")\n",
        "print(f\"\\nModel type: {type(model).__name__}\")\n",
        "print(f\"Number of trees: {model.n_estimators}\")\n",
        "print(f\"Max depth: {model.max_depth}\")"
      ],
      "metadata": {
        "id": "train-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Probability of adverse outcome\n",
        "\n",
        "# Add predictions to demographics dataframe for analysis\n",
        "demographics_test['true_outcome'] = y_test.values\n",
        "demographics_test['predicted_outcome'] = y_pred\n",
        "demographics_test['predicted_probability'] = y_prob\n",
        "\n",
        "print(\"Predictions generated!\")\n",
        "print(f\"Predicted positive rate: {y_pred.mean()*100:.1f}%\")\n",
        "print(f\"Actual positive rate: {y_test.mean()*100:.1f}%\")"
      ],
      "metadata": {
        "id": "generate-predictions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Overall Model Performance\n",
        "\n",
        "Before examining fairness, let's understand how well the model performs overall."
      ],
      "metadata": {
        "id": "part3-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate overall performance metrics\n",
        "print(\"=\"*60)\n",
        "print(\"OVERALL MODEL PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "overall_metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred),\n",
        "    'Precision (PPV)': precision_score(y_test, y_pred),\n",
        "    'Recall (Sensitivity)': recall_score(y_test, y_pred),\n",
        "    'AUC-ROC': roc_auc_score(y_test, y_prob)\n",
        "}\n",
        "\n",
        "for metric, value in overall_metrics.items():\n",
        "    print(f\"  {metric}: {value:.3f}\")\n",
        "\n",
        "print(\"\\nüí° Interpretation:\")\n",
        "print(f\"  ‚Ä¢ The model correctly identifies {overall_metrics['Recall (Sensitivity)']*100:.0f}% of patients who will deteriorate\")\n",
        "print(f\"  ‚Ä¢ When it predicts deterioration, it's correct {overall_metrics['Precision (PPV)']*100:.0f}% of the time\")"
      ],
      "metadata": {
        "id": "overall-metrics"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['No Deterioration', 'Deterioration'],\n",
        "            yticklabels=['No Deterioration', 'Deterioration'])\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Confusion Matrix')\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {overall_metrics[\"AUC-ROC\"]:.3f}')\n",
        "axes[1].plot([0, 1], [0, 1], 'r--', label='Random classifier')\n",
        "axes[1].set_xlabel('False Positive Rate (1 - Specificity)')\n",
        "axes[1].set_ylabel('True Positive Rate (Sensitivity)')\n",
        "axes[1].set_title('ROC Curve')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Key Question: Does this 'good' overall performance hold for all patient groups?\")"
      ],
      "metadata": {
        "id": "confusion-matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Stratified Performance Analysis\n",
        "\n",
        "Now let's examine whether performance varies across demographic groups. This is the heart of algorithmic fairness analysis."
      ],
      "metadata": {
        "id": "part4-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_group_metrics(df, group_column):\n",
        "    \"\"\"\n",
        "    Calculate performance metrics for each subgroup.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for group_value in df[group_column].unique():\n",
        "        mask = df[group_column] == group_value\n",
        "        subset = df[mask]\n",
        "        \n",
        "        n = len(subset)\n",
        "        if n < 10:  # Skip very small groups\n",
        "            continue\n",
        "            \n",
        "        y_true = subset['true_outcome']\n",
        "        y_pred = subset['predicted_outcome']\n",
        "        y_prob = subset['predicted_probability']\n",
        "        \n",
        "        # Calculate metrics (handle edge cases)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true, y_prob)\n",
        "        except:\n",
        "            auc = np.nan\n",
        "            \n",
        "        metrics = {\n",
        "            'Group': group_value,\n",
        "            'N': n,\n",
        "            'Base Rate': y_true.mean(),\n",
        "            'Accuracy': accuracy_score(y_true, y_pred),\n",
        "            'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'AUC': auc,\n",
        "            'Positive Rate': y_pred.mean()  # For demographic parity\n",
        "        }\n",
        "        results.append(metrics)\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Calculate metrics for each demographic grouping\n",
        "print(\"Calculating stratified performance metrics...\\n\")"
      ],
      "metadata": {
        "id": "stratified-function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance by Indigenous Status\n",
        "print(\"=\"*70)\n",
        "print(\"PERFORMANCE BY INDIGENOUS STATUS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "indigenous_metrics = calculate_group_metrics(demographics_test, 'indigenous_grouped')\n",
        "\n",
        "# Format for display\n",
        "display_cols = ['Group', 'N', 'Base Rate', 'Recall', 'Precision', 'AUC']\n",
        "display_df = indigenous_metrics[display_cols].copy()\n",
        "display_df['Base Rate'] = (display_df['Base Rate'] * 100).round(1).astype(str) + '%'\n",
        "display_df['Recall'] = (display_df['Recall'] * 100).round(1).astype(str) + '%'\n",
        "display_df['Precision'] = (display_df['Precision'] * 100).round(1).astype(str) + '%'\n",
        "display_df['AUC'] = display_df['AUC'].round(3)\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "# Calculate disparity\n",
        "if len(indigenous_metrics) > 1:\n",
        "    max_recall = indigenous_metrics['Recall'].max()\n",
        "    min_recall = indigenous_metrics['Recall'].min()\n",
        "    print(f\"\\n‚ö†Ô∏è Recall disparity: {max_recall*100:.1f}% vs {min_recall*100:.1f}%\")\n",
        "    print(f\"   Ratio: {max_recall/min_recall:.2f}x\")"
      ],
      "metadata": {
        "id": "indigenous-performance"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance by Remoteness\n",
        "print(\"=\"*70)\n",
        "print(\"PERFORMANCE BY REMOTENESS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "remoteness_metrics = calculate_group_metrics(demographics_test, 'remoteness_grouped')\n",
        "\n",
        "display_df = remoteness_metrics[display_cols].copy()\n",
        "display_df['Base Rate'] = (display_df['Base Rate'] * 100).round(1).astype(str) + '%'\n",
        "display_df['Recall'] = (display_df['Recall'] * 100).round(1).astype(str) + '%'\n",
        "display_df['Precision'] = (display_df['Precision'] * 100).round(1).astype(str) + '%'\n",
        "display_df['AUC'] = display_df['AUC'].round(3)\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "if len(remoteness_metrics) > 1:\n",
        "    max_recall = remoteness_metrics['Recall'].max()\n",
        "    min_recall = remoteness_metrics['Recall'].min()\n",
        "    print(f\"\\n‚ö†Ô∏è Recall disparity: {max_recall*100:.1f}% vs {min_recall*100:.1f}%\")"
      ],
      "metadata": {
        "id": "remoteness-performance"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance by Socioeconomic Status\n",
        "print(\"=\"*70)\n",
        "print(\"PERFORMANCE BY SOCIOECONOMIC STATUS (SEIFA)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "seifa_metrics = calculate_group_metrics(demographics_test, 'seifa_grouped')\n",
        "\n",
        "display_df = seifa_metrics[display_cols].copy()\n",
        "display_df['Base Rate'] = (display_df['Base Rate'] * 100).round(1).astype(str) + '%'\n",
        "display_df['Recall'] = (display_df['Recall'] * 100).round(1).astype(str) + '%'\n",
        "display_df['Precision'] = (display_df['Precision'] * 100).round(1).astype(str) + '%'\n",
        "display_df['AUC'] = display_df['AUC'].round(3)\n",
        "\n",
        "print(display_df.to_string(index=False))\n",
        "\n",
        "if len(seifa_metrics) > 1:\n",
        "    max_recall = seifa_metrics['Recall'].max()\n",
        "    min_recall = seifa_metrics['Recall'].min()\n",
        "    print(f\"\\n‚ö†Ô∏è Recall disparity: {max_recall*100:.1f}% vs {min_recall*100:.1f}%\")"
      ],
      "metadata": {
        "id": "seifa-performance"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise performance disparities\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Indigenous status\n",
        "indigenous_metrics_sorted = indigenous_metrics.sort_values('Recall', ascending=True)\n",
        "axes[0].barh(indigenous_metrics_sorted['Group'], indigenous_metrics_sorted['Recall'] * 100, color='steelblue')\n",
        "axes[0].axvline(overall_metrics['Recall (Sensitivity)'] * 100, color='red', linestyle='--', label='Overall')\n",
        "axes[0].set_xlabel('Recall (%)')\n",
        "axes[0].set_title('Recall by Indigenous Status')\n",
        "axes[0].legend()\n",
        "\n",
        "# Remoteness\n",
        "remoteness_metrics_sorted = remoteness_metrics.sort_values('Recall', ascending=True)\n",
        "axes[1].barh(remoteness_metrics_sorted['Group'], remoteness_metrics_sorted['Recall'] * 100, color='steelblue')\n",
        "axes[1].axvline(overall_metrics['Recall (Sensitivity)'] * 100, color='red', linestyle='--', label='Overall')\n",
        "axes[1].set_xlabel('Recall (%)')\n",
        "axes[1].set_title('Recall by Remoteness')\n",
        "axes[1].legend()\n",
        "\n",
        "# SEIFA\n",
        "seifa_metrics_sorted = seifa_metrics.sort_values('Recall', ascending=True)\n",
        "axes[2].barh(seifa_metrics_sorted['Group'].astype(str), seifa_metrics_sorted['Recall'] * 100, color='steelblue')\n",
        "axes[2].axvline(overall_metrics['Recall (Sensitivity)'] * 100, color='red', linestyle='--', label='Overall')\n",
        "axes[2].set_xlabel('Recall (%)')\n",
        "axes[2].set_title('Recall by SEIFA Group')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Key Insight: The model may have good OVERALL performance but\")\n",
        "print(\"   systematically miss deteriorating patients in certain groups.\")"
      ],
      "metadata": {
        "id": "visualise-disparities"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Formal Fairness Metrics\n",
        "\n",
        "Now let's apply formal fairness definitions. There are several ways to define \"fair\", and they can conflict with each other."
      ],
      "metadata": {
        "id": "part5-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fairness metric calculations\n",
        "\n",
        "def calculate_fairness_metrics(df, group_column, reference_group=None):\n",
        "    \"\"\"\n",
        "    Calculate common fairness metrics.\n",
        "    \"\"\"\n",
        "    groups = df[group_column].unique()\n",
        "    \n",
        "    if reference_group is None:\n",
        "        # Use largest group as reference\n",
        "        reference_group = df[group_column].value_counts().idxmax()\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for group in groups:\n",
        "        mask = df[group_column] == group\n",
        "        subset = df[mask]\n",
        "        \n",
        "        # Positive prediction rate (for demographic parity)\n",
        "        positive_rate = subset['predicted_outcome'].mean()\n",
        "        \n",
        "        # True positive rate (recall) and false positive rate (for equalised odds)\n",
        "        positives = subset[subset['true_outcome'] == 1]\n",
        "        negatives = subset[subset['true_outcome'] == 0]\n",
        "        \n",
        "        tpr = positives['predicted_outcome'].mean() if len(positives) > 0 else np.nan\n",
        "        fpr = negatives['predicted_outcome'].mean() if len(negatives) > 0 else np.nan\n",
        "        \n",
        "        results[group] = {\n",
        "            'N': len(subset),\n",
        "            'Positive_Rate': positive_rate,\n",
        "            'TPR': tpr,\n",
        "            'FPR': fpr\n",
        "        }\n",
        "    \n",
        "    return pd.DataFrame(results).T, reference_group"
      ],
      "metadata": {
        "id": "fairness-functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate fairness metrics by Indigenous status\n",
        "print(\"=\"*70)\n",
        "print(\"FAIRNESS METRICS BY INDIGENOUS STATUS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fairness_indigenous, ref_group = calculate_fairness_metrics(\n",
        "    demographics_test, 'indigenous_grouped', 'Non-Indigenous'\n",
        ")\n",
        "\n",
        "print(f\"\\nReference group: {ref_group}\")\n",
        "print(\"\\nMetrics by group:\")\n",
        "print(fairness_indigenous.round(3))\n",
        "\n",
        "# Calculate disparities\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"FAIRNESS ANALYSIS:\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "ref_metrics = fairness_indigenous.loc[ref_group]\n",
        "\n",
        "for group in fairness_indigenous.index:\n",
        "    if group != ref_group:\n",
        "        group_metrics = fairness_indigenous.loc[group]\n",
        "        \n",
        "        # Demographic Parity Ratio\n",
        "        dp_ratio = group_metrics['Positive_Rate'] / ref_metrics['Positive_Rate']\n",
        "        \n",
        "        # Equalised Odds - TPR and FPR differences\n",
        "        tpr_diff = group_metrics['TPR'] - ref_metrics['TPR']\n",
        "        fpr_diff = group_metrics['FPR'] - ref_metrics['FPR']\n",
        "        \n",
        "        print(f\"\\n{group} vs {ref_group}:\")\n",
        "        print(f\"  Demographic Parity Ratio: {dp_ratio:.2f}\")\n",
        "        print(f\"    (1.0 = equal positive prediction rates)\")\n",
        "        print(f\"  TPR Difference: {tpr_diff:+.3f}\")\n",
        "        print(f\"  FPR Difference: {fpr_diff:+.3f}\")\n",
        "        print(f\"    (0.0 = equal error rates = Equalised Odds)\")"
      ],
      "metadata": {
        "id": "fairness-indigenous"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise fairness metrics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Demographic Parity - Positive prediction rates\n",
        "axes[0].bar(fairness_indigenous.index, fairness_indigenous['Positive_Rate'] * 100, \n",
        "            color='steelblue', edgecolor='white')\n",
        "axes[0].axhline(y=demographics_test['predicted_outcome'].mean() * 100, \n",
        "                color='red', linestyle='--', label='Overall rate')\n",
        "axes[0].set_ylabel('Positive Prediction Rate (%)')\n",
        "axes[0].set_title('Demographic Parity\\n(Equal Prediction Rates)')\n",
        "axes[0].legend()\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# True Positive Rate (Recall)\n",
        "axes[1].bar(fairness_indigenous.index, fairness_indigenous['TPR'] * 100,\n",
        "            color='forestgreen', edgecolor='white')\n",
        "axes[1].axhline(y=overall_metrics['Recall (Sensitivity)'] * 100,\n",
        "                color='red', linestyle='--', label='Overall rate')\n",
        "axes[1].set_ylabel('True Positive Rate / Recall (%)')\n",
        "axes[1].set_title('Equal Opportunity\\n(Equal TPR for True Positives)')\n",
        "axes[1].legend()\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# False Positive Rate\n",
        "axes[2].bar(fairness_indigenous.index, fairness_indigenous['FPR'] * 100,\n",
        "            color='coral', edgecolor='white')\n",
        "axes[2].set_ylabel('False Positive Rate (%)')\n",
        "axes[2].set_title('Predictive Equality\\n(Equal FPR for True Negatives)')\n",
        "axes[2].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìö Fairness Definitions:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"‚Ä¢ Demographic Parity: Equal positive prediction rates across groups\")\n",
        "print(\"‚Ä¢ Equal Opportunity: Equal TPR (detecting true positives) across groups\")\n",
        "print(\"‚Ä¢ Equalised Odds: Equal TPR AND FPR across groups\")\n",
        "print(\"\\n‚ö†Ô∏è These definitions can conflict - you may need to choose which matters most!\")"
      ],
      "metadata": {
        "id": "visualise-fairness"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Your Turn: Analyse Fairness by SEIFA Group"
      ],
      "metadata": {
        "id": "exercise-fairness"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE: Calculate and interpret fairness metrics by SEIFA group\n",
        "# Use the calculate_fairness_metrics function\n",
        "\n",
        "fairness_seifa, ref_group = calculate_fairness_metrics(\n",
        "    demographics_test, 'seifa_grouped', 'High (8-10)'\n",
        ")\n",
        "\n",
        "print(\"Fairness Metrics by SEIFA Group:\")\n",
        "print(fairness_seifa.round(3))\n",
        "\n",
        "# Add your interpretation here:\n",
        "# What disparities do you observe?\n",
        "# Which groups might be disadvantaged by this model?"
      ],
      "metadata": {
        "id": "your-fairness-analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: The Fairness Trade-off Challenge\n",
        "\n",
        "Here's a crucial insight: **different fairness criteria often conflict**. Let's explore why."
      ],
      "metadata": {
        "id": "part6-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate the impossibility theorem\n",
        "print(\"=\"*70)\n",
        "print(\"THE FAIRNESS TRADE-OFF DILEMMA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "Consider two groups with DIFFERENT base rates of adverse outcomes:\n",
        "\n",
        "  Group A: 30% adverse outcome rate (higher risk population)\n",
        "  Group B: 15% adverse outcome rate (lower risk population)\n",
        "\n",
        "A well-calibrated model should predict higher risk for Group A.\n",
        "But this VIOLATES demographic parity (equal positive prediction rates).\n",
        "\n",
        "If we FORCE demographic parity:\n",
        "  - We might under-predict risk for Group A (missing deteriorating patients)\n",
        "  - Or over-predict risk for Group B (unnecessary interventions)\n",
        "\n",
        "This is known as the \"impossibility theorem\" of algorithmic fairness.\n",
        "When base rates differ, you CANNOT satisfy all fairness criteria simultaneously.\n",
        "\"\"\")\n",
        "\n",
        "# Show base rates in our data\n",
        "print(\"\\nBase rates in our dataset:\")\n",
        "print(\"-\" * 40)\n",
        "for group in demographics_test['seifa_grouped'].unique():\n",
        "    rate = demographics_test[demographics_test['seifa_grouped'] == group]['true_outcome'].mean()\n",
        "    print(f\"  {group}: {rate*100:.1f}%\")"
      ],
      "metadata": {
        "id": "fairness-tradeoff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore threshold adjustment as mitigation\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MITIGATION EXPLORATION: Threshold Adjustment\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Currently using 0.5 threshold for all groups\n",
        "# What if we use different thresholds per group?\n",
        "\n",
        "def apply_threshold(prob, threshold):\n",
        "    return (prob >= threshold).astype(int)\n",
        "\n",
        "# Standard threshold\n",
        "standard_threshold = 0.5\n",
        "\n",
        "print(f\"\\nStandard threshold ({standard_threshold}) for all groups:\")\n",
        "for group in ['Indigenous', 'Non-Indigenous']:\n",
        "    if group == 'Not Stated':\n",
        "        continue\n",
        "    mask = demographics_test['indigenous_grouped'] == group\n",
        "    subset = demographics_test[mask]\n",
        "    pred = apply_threshold(subset['predicted_probability'], standard_threshold)\n",
        "    recall = recall_score(subset['true_outcome'], pred, zero_division=0)\n",
        "    print(f\"  {group}: Recall = {recall*100:.1f}%\")\n",
        "\n",
        "print(\"\\nüí° Group-specific thresholds could equalise recall,\")\n",
        "print(\"   but this raises ethical questions about treating groups differently.\")"
      ],
      "metadata": {
        "id": "mitigation-exploration"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Governance Decision Framework\n",
        "\n",
        "Based on your analysis, you need to make a recommendation to the clinical governance committee."
      ],
      "metadata": {
        "id": "part7-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate governance summary report\n",
        "print(\"=\"*70)\n",
        "print(\"FAIRNESS ASSESSMENT SUMMARY FOR GOVERNANCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä OVERALL MODEL PERFORMANCE:\")\n",
        "print(\"-\" * 50)\n",
        "for metric, value in overall_metrics.items():\n",
        "    print(f\"  {metric}: {value:.3f}\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è IDENTIFIED DISPARITIES:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Indigenous\n",
        "ind_recall = indigenous_metrics.set_index('Group')['Recall']\n",
        "if 'Indigenous' in ind_recall.index and 'Non-Indigenous' in ind_recall.index:\n",
        "    gap = ind_recall['Non-Indigenous'] - ind_recall['Indigenous']\n",
        "    print(f\"  Indigenous vs Non-Indigenous recall gap: {gap*100:+.1f} percentage points\")\n",
        "\n",
        "# Remoteness\n",
        "rem_recall = remoteness_metrics.set_index('Group')['Recall']\n",
        "if 'Remote' in rem_recall.index and 'Non-Remote' in rem_recall.index:\n",
        "    gap = rem_recall['Non-Remote'] - rem_recall['Remote']\n",
        "    print(f\"  Remote vs Non-Remote recall gap: {gap*100:+.1f} percentage points\")\n",
        "\n",
        "# SEIFA\n",
        "seifa_recall = seifa_metrics.set_index('Group')['Recall']\n",
        "if 'Low (1-3)' in seifa_recall.index and 'High (8-10)' in seifa_recall.index:\n",
        "    gap = seifa_recall['High (8-10)'] - seifa_recall['Low (1-3)']\n",
        "    print(f\"  Low vs High SEIFA recall gap: {gap*100:+.1f} percentage points\")\n",
        "\n",
        "print(\"\\nüìã CLINICAL IMPLICATIONS:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"  ‚Ä¢ If deployed, the model may miss more deteriorating patients in:\")\n",
        "print(\"    - Indigenous populations\")\n",
        "print(\"    - Remote/rural areas\")\n",
        "print(\"    - Lower socioeconomic areas\")\n",
        "print(\"  ‚Ä¢ These are often the populations with least access to alternatives\")\n",
        "\n",
        "print(\"\\n‚ùì GOVERNANCE QUESTIONS:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"  1. What level of disparity is acceptable?\")\n",
        "print(\"  2. Who decides what 'fair enough' means?\")\n",
        "print(\"  3. Should we delay deployment until disparities are addressed?\")\n",
        "print(\"  4. Are there mitigation strategies we can implement?\")\n",
        "print(\"  5. How will we monitor for disparities post-deployment?\")"
      ],
      "metadata": {
        "id": "governance-summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 8: Your Governance Recommendation\n",
        "\n",
        "Based on your analysis, complete the governance recommendation below."
      ],
      "metadata": {
        "id": "part8-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== YOUR GOVERNANCE RECOMMENDATION =====\n",
        "\n",
        "governance_recommendation = \"\"\"\n",
        "ALGORITHMIC FAIRNESS ASSESSMENT\n",
        "AI System: Deterioration Prediction Model\n",
        "Date: [Your date]\n",
        "Assessor: [Your name]\n",
        "\n",
        "============================================================\n",
        "\n",
        "1. EXECUTIVE SUMMARY\n",
        "------------------------------------------------------------\n",
        "[Summarise overall performance and key fairness findings in 2-3 sentences]\n",
        "\n",
        "\n",
        "\n",
        "2. KEY FINDINGS\n",
        "------------------------------------------------------------\n",
        "Overall Performance:\n",
        "  - AUC: [value]\n",
        "  - Recall: [value]\n",
        "\n",
        "Identified Disparities:\n",
        "  - [Disparity 1]\n",
        "  - [Disparity 2]\n",
        "  - [Disparity 3]\n",
        "\n",
        "3. CLINICAL IMPACT ASSESSMENT\n",
        "------------------------------------------------------------\n",
        "[What are the real-world implications of these disparities?]\n",
        "[Which patient groups are most affected?]\n",
        "[What are the consequences of missed deterioration?]\n",
        "\n",
        "\n",
        "\n",
        "4. RECOMMENDATION\n",
        "------------------------------------------------------------\n",
        "[ ] APPROVE for deployment without conditions\n",
        "[ ] APPROVE with conditions (specify below)\n",
        "[ ] DEFER pending further evaluation\n",
        "[ ] DO NOT APPROVE\n",
        "\n",
        "Conditions/Rationale:\n",
        "\n",
        "\n",
        "\n",
        "5. PROPOSED MITIGATIONS\n",
        "------------------------------------------------------------\n",
        "[If recommending approval, what mitigations should be implemented?]\n",
        "\n",
        "\n",
        "\n",
        "6. MONITORING REQUIREMENTS\n",
        "------------------------------------------------------------\n",
        "[How should fairness be monitored post-deployment?]\n",
        "\n",
        "\n",
        "\n",
        "============================================================\n",
        "\"\"\"\n",
        "\n",
        "print(governance_recommendation)"
      ],
      "metadata": {
        "id": "your-recommendation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 9: Reflection Questions"
      ],
      "metadata": {
        "id": "reflection-intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== YOUR REFLECTIONS =====\n",
        "\n",
        "reflections = \"\"\"\n",
        "1. If the model has lower recall for Indigenous patients, what are the\n",
        "   clinical implications? Who bears the cost of this disparity?\n",
        "   Your answer:\n",
        "   \n",
        "\n",
        "2. Is a 5% difference in AUC between groups acceptable? What about 10%?\n",
        "   Who should decide this threshold?\n",
        "   Your answer:\n",
        "   \n",
        "\n",
        "3. If disparities exist, is the problem:\n",
        "   a) The model itself?\n",
        "   b) The training data?\n",
        "   c) The underlying healthcare system?\n",
        "   d) All of the above?\n",
        "   Your answer:\n",
        "   \n",
        "\n",
        "4. Should we deploy an imperfect model if it still improves on current\n",
        "   practice overall, even if it worsens disparities?\n",
        "   Your answer:\n",
        "   \n",
        "\n",
        "5. What role should affected communities play in these decisions?\n",
        "   Your answer:\n",
        "   \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(reflections)"
      ],
      "metadata": {
        "id": "reflections"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Deliverable\n",
        "\n",
        "**For your portfolio:**\n",
        "\n",
        "Complete the governance recommendation (Part 8) with:\n",
        "1. Your fairness analysis findings\n",
        "2. A clear recommendation (approve/defer/reject)\n",
        "3. Proposed mitigations or conditions\n",
        "4. Monitoring requirements\n",
        "\n",
        "**Word count:** Approximately 500 words for the governance recommendation.\n",
        "\n",
        "Submit via LMS by the Week 5 deadline."
      ],
      "metadata": {
        "id": "deliverable"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèÅ Summary\n",
        "\n",
        "In this exercise, you learned:\n",
        "\n",
        "‚úÖ **Overall performance can hide disparities** - always stratify by subgroup\n",
        "\n",
        "‚úÖ **Multiple fairness definitions exist** - demographic parity, equal opportunity, equalised odds\n",
        "\n",
        "‚úÖ **Fairness criteria can conflict** - the impossibility theorem means trade-offs are unavoidable\n",
        "\n",
        "‚úÖ **Context matters** - what's \"fair enough\" depends on clinical impact and alternatives\n",
        "\n",
        "‚úÖ **Governance decisions are ethical decisions** - they should involve diverse perspectives\n",
        "\n",
        "**Key takeaway:** Algorithmic fairness is not just a technical problem‚Äîit requires human judgment about acceptable trade-offs and who should make those decisions.\n",
        "\n",
        "---\n",
        "\n",
        "**Next exercise (Week 7):** We'll examine how to validate a vendor model on your local population before deployment."
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}
