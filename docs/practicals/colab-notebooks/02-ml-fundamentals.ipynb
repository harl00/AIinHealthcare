{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üß† Exercise 2: Machine Learning Fundamentals\n",
        "\n",
        "**Week 2 | AI in Healthcare Curriculum**\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this exercise, you will:\n",
        "\n",
        "- üéØ Understand how ML models learn from data\n",
        "- üéØ Train your own classification model\n",
        "- üéØ Understand train/test splitting and why it matters\n",
        "- üéØ See how changing parameters affects model behaviour\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è±Ô∏è Estimated Time: 2 hours\n",
        "\n",
        "---\n",
        "\n",
        "## Context\n",
        "\n",
        "Last week, you used pre-trained AI models. This week, you'll train one yourself from scratch. This demystifies the \"learning\" in machine learning.\n",
        "\n",
        "**Our scenario:** We'll build a model to predict which patients might deteriorate within 24 hours, using vital signs data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1"
      },
      "source": [
        "## Part 1: Setup and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Load the synthetic dataset\n",
        "url = \"https://raw.githubusercontent.com/harl00/AIinHealthcare/main/data/AI_in_HealthCare_Dataset.csv\"\n",
        "ed_data = pd.read_csv(url)\n",
        "ed_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-intro"
      },
      "source": [
        "### Creating Our Dataset\n",
        "\n",
        "In real healthcare AI, training data comes from electronic health records. For this exercise, we'll create realistic synthetic data.\n",
        "\n",
        "**What we're simulating:**\n",
        "- 1000 patient encounters\n",
        "- Vital signs at a point in time\n",
        "- Whether the patient deteriorated in the next 24 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate-data"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic patient data\n",
        "# This function creates realistic-looking vital signs data\n",
        "\n",
        "def generate_patient_data(n_patients=1000):\n",
        "    \"\"\"\n",
        "    Generate synthetic patient vital signs data.\n",
        "\n",
        "    In real life, this would come from your EHR system.\n",
        "    We're creating synthetic data that has realistic patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate vital signs with realistic distributions\n",
        "    data = {\n",
        "        'patient_id': [f'P{i:04d}' for i in range(n_patients)],\n",
        "        'age': np.random.normal(60, 18, n_patients).clip(18, 95).astype(int),\n",
        "        'heart_rate': np.random.normal(82, 18, n_patients).clip(40, 180).astype(int),\n",
        "        'respiratory_rate': np.random.normal(17, 5, n_patients).clip(8, 40).astype(int),\n",
        "        'systolic_bp': np.random.normal(125, 22, n_patients).clip(70, 200).astype(int),\n",
        "        'diastolic_bp': np.random.normal(75, 12, n_patients).clip(40, 120).astype(int),\n",
        "        'temperature': np.round(np.random.normal(37.0, 0.7, n_patients).clip(35, 40), 1),\n",
        "        'oxygen_saturation': np.random.normal(96, 3, n_patients).clip(80, 100).astype(int),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Create outcome based on vital sign patterns\n",
        "    # This simulates a realistic relationship between vitals and deterioration\n",
        "    risk_score = (\n",
        "        0.03 * (df['heart_rate'] - 80) +\n",
        "        0.08 * (df['respiratory_rate'] - 16) +\n",
        "        -0.02 * (df['systolic_bp'] - 120) +\n",
        "        0.5 * (df['temperature'] - 37) +\n",
        "        -0.1 * (df['oxygen_saturation'] - 96) +\n",
        "        0.02 * (df['age'] - 60) +\n",
        "        np.random.normal(0, 0.5, n_patients)  # Random noise\n",
        "    )\n",
        "\n",
        "    # Convert to binary outcome (deteriorated or not)\n",
        "    df['deteriorated'] = (risk_score > 0.8).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generate the data\n",
        "patient_data = generate_patient_data(1000)\n",
        "\n",
        "print(\"Patient Dataset Generated\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total patients: {len(patient_data)}\")\n",
        "print(f\"\\nOutcome distribution:\")\n",
        "print(f\"  Did NOT deteriorate: {(patient_data['deteriorated']==0).sum()} ({(patient_data['deteriorated']==0).mean()*100:.1f}%)\")\n",
        "print(f\"  DID deteriorate: {(patient_data['deteriorated']==1).sum()} ({(patient_data['deteriorated']==1).mean()*100:.1f}%)\")\n",
        "print(\"\\nFirst 10 patients:\")\n",
        "patient_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "explore-intro"
      },
      "source": [
        "### Exploring the Data\n",
        "\n",
        "Before training a model, data scientists always explore the data first. Let's see what we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary-stats"
      },
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"Summary Statistics:\")\n",
        "print(\"=\"*60)\n",
        "patient_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare-groups"
      },
      "outputs": [],
      "source": [
        "# Compare vital signs between groups\n",
        "print(\"Vital Signs by Outcome Group:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison = patient_data.groupby('deteriorated').agg({\n",
        "    'age': 'mean',\n",
        "    'heart_rate': 'mean',\n",
        "    'respiratory_rate': 'mean',\n",
        "    'systolic_bp': 'mean',\n",
        "    'temperature': 'mean',\n",
        "    'oxygen_saturation': 'mean'\n",
        "}).round(1)\n",
        "\n",
        "comparison.index = ['Did NOT deteriorate', 'DID deteriorate']\n",
        "comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "observation-1"
      },
      "source": [
        "### üí° Observation\n",
        "\n",
        "Look at the differences between the groups:\n",
        "- Patients who deteriorated had **higher** heart rates, respiratory rates, and temperatures\n",
        "- Patients who deteriorated had **lower** blood pressure and oxygen saturation\n",
        "\n",
        "**This is exactly what a ML model will learn to detect!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2-intro"
      },
      "source": [
        "## Part 2: The Critical Concept - Train/Test Split\n",
        "\n",
        "### Why Can't We Test on the Same Data We Train On?\n",
        "\n",
        "Imagine studying for an exam by memorising the exact questions and answers. You'd score 100%... on those specific questions. But could you answer *new* questions?\n",
        "\n",
        "ML models can \"memorise\" training data too. This is called **overfitting**.\n",
        "\n",
        "**Solution:** Split the data:\n",
        "- **Training set** - The model learns from this\n",
        "- **Test set** - We evaluate on this (model has never seen it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare-data"
      },
      "outputs": [],
      "source": [
        "# Prepare features (X) and labels (y)\n",
        "\n",
        "# Features: the vital signs we'll use to make predictions\n",
        "feature_columns = ['age', 'heart_rate', 'respiratory_rate',\n",
        "                   'systolic_bp', 'temperature', 'oxygen_saturation']\n",
        "\n",
        "X = patient_data[feature_columns]  # Features\n",
        "y = patient_data['deteriorated']    # Labels (what we're predicting)\n",
        "\n",
        "print(\"Feature matrix (X):\")\n",
        "print(f\"  Shape: {X.shape} (rows=patients, columns=features)\")\n",
        "print(f\"  Features: {list(X.columns)}\")\n",
        "print(f\"\\nLabel vector (y):\")\n",
        "print(f\"  Shape: {y.shape}\")\n",
        "print(f\"  Values: 0 (no deterioration) or 1 (deterioration)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-test-split"
      },
      "outputs": [],
      "source": [
        "# Split into training and test sets\n",
        "# We'll use 80% for training, 20% for testing\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.20,      # 20% for testing\n",
        "    random_state=42,     # For reproducibility\n",
        "    stratify=y           # Keep same proportion of outcomes in both sets\n",
        ")\n",
        "\n",
        "print(\"Data Split Complete\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nTraining set: {len(X_train)} patients ({len(X_train)/len(X)*100:.0f}%)\")\n",
        "print(f\"  - Did not deteriorate: {(y_train==0).sum()}\")\n",
        "print(f\"  - Did deteriorate: {(y_train==1).sum()}\")\n",
        "\n",
        "print(f\"\\nTest set: {len(X_test)} patients ({len(X_test)/len(X)*100:.0f}%)\")\n",
        "print(f\"  - Did not deteriorate: {(y_test==0).sum()}\")\n",
        "print(f\"  - Did deteriorate: {(y_test==1).sum()}\")\n",
        "\n",
        "print(\"\\n‚úÖ The model will ONLY learn from training data.\")\n",
        "print(\"‚úÖ We'll evaluate on test data the model has NEVER seen.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3-intro"
      },
      "source": [
        "## Part 3: Training Your First Model - Decision Tree\n",
        "\n",
        "We'll start with a **Decision Tree** - one of the most interpretable ML models.\n",
        "\n",
        "A decision tree asks a series of yes/no questions to classify patients:\n",
        "- \"Is heart rate > 100?\" ‚Üí If yes, go left; if no, go right\n",
        "- Continue until reaching a conclusion\n",
        "\n",
        "This is similar to how clinicians think through differential diagnoses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-tree"
      },
      "outputs": [],
      "source": [
        "# Train a Decision Tree classifier\n",
        "\n",
        "# Create the model\n",
        "# max_depth=4 limits how many questions deep the tree can go\n",
        "tree_model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "\n",
        "# Train (fit) the model on training data\n",
        "# This is where the \"learning\" happens!\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"‚úÖ Decision Tree trained!\")\n",
        "print(f\"\\nThe model learned to ask {tree_model.get_n_leaves()} different questions\")\n",
        "print(f\"to classify patients into {tree_model.n_classes_} categories.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualise-tree"
      },
      "source": [
        "### Visualising the Decision Tree\n",
        "\n",
        "One huge advantage of decision trees is we can SEE what they learned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-tree"
      },
      "outputs": [],
      "source": [
        "# Visualise the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(tree_model,\n",
        "          feature_names=feature_columns,\n",
        "          class_names=['No Deterioration', 'Deterioration'],\n",
        "          filled=True,\n",
        "          rounded=True,\n",
        "          fontsize=10)\n",
        "plt.title('Decision Tree for Deterioration Prediction', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä How to read this tree:\")\n",
        "print(\"  - Start at the top (root node)\")\n",
        "print(\"  - Each box shows a question and the split\")\n",
        "print(\"  - Follow left branch if condition is TRUE, right if FALSE\")\n",
        "print(\"  - Colour: blue = predicts no deterioration, orange = predicts deterioration\")\n",
        "print(\"  - Darker colour = more confident prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tree-reflection"
      },
      "source": [
        "### üí° What Did the Model Learn?\n",
        "\n",
        "Look at the tree and answer:\n",
        "1. What's the first (most important) question the model asks?\n",
        "2. Does this make clinical sense?\n",
        "3. Can you trace through what happens to a patient with:\n",
        "   - Respiratory rate = 25, Temperature = 38.0, Oxygen sat = 90?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4-intro"
      },
      "source": [
        "## Part 4: Evaluating Model Performance\n",
        "\n",
        "Now the crucial question: **How well does our model actually work?**\n",
        "\n",
        "Remember: We must test on data the model has never seen!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "y_pred = tree_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance on TEST DATA\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nAccuracy: {accuracy*100:.1f}%\")\n",
        "print(f\"\\nThis means the model correctly classified {accuracy*100:.1f}% of patients\")\n",
        "print(f\"it had never seen before.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "confusion-intro"
      },
      "source": [
        "### The Confusion Matrix\n",
        "\n",
        "Accuracy alone doesn't tell the whole story. In healthcare, we care about:\n",
        "- **False Negatives:** Missed deteriorating patients (dangerous!)\n",
        "- **False Positives:** Unnecessary alerts (alert fatigue)\n",
        "\n",
        "A **confusion matrix** shows all four outcomes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "confusion-matrix"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualise it\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Create heatmap\n",
        "im = ax.imshow(cm, cmap='Blues')\n",
        "\n",
        "# Labels\n",
        "ax.set_xticks([0, 1])\n",
        "ax.set_yticks([0, 1])\n",
        "ax.set_xticklabels(['Predicted: No', 'Predicted: Yes'], fontsize=12)\n",
        "ax.set_yticklabels(['Actual: No', 'Actual: Yes'], fontsize=12)\n",
        "ax.set_xlabel('Model Prediction', fontsize=14)\n",
        "ax.set_ylabel('Actual Outcome', fontsize=14)\n",
        "ax.set_title('Confusion Matrix: Deterioration Prediction', fontsize=16)\n",
        "\n",
        "# Add numbers\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        color = 'white' if cm[i, j] > cm.max()/2 else 'black'\n",
        "        ax.text(j, i, str(cm[i, j]), ha='center', va='center',\n",
        "                fontsize=24, fontweight='bold', color=color)\n",
        "\n",
        "# Add labels for each quadrant\n",
        "labels = [['True Negative\\n(Correct: No deterioration)', 'False Positive\\n(Alert fatigue)'],\n",
        "          ['False Negative\\n(MISSED deterioration!)', 'True Positive\\n(Correct: Caught deterioration)']]\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        ax.text(j, i+0.35, labels[i][j], ha='center', va='center',\n",
        "                fontsize=9, color='darkgray')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "sensitivity = tp / (tp + fn)  # Also called \"recall\"\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "print(\"\\nDetailed Metrics:\")\n",
        "print(f\"  True Negatives: {tn} (correctly said NO deterioration)\")\n",
        "print(f\"  True Positives: {tp} (correctly caught deterioration)\")\n",
        "print(f\"  False Negatives: {fn} (MISSED deterioration - dangerous!)\")\n",
        "print(f\"  False Positives: {fp} (false alarms)\")\n",
        "print(f\"\\n  Sensitivity: {sensitivity*100:.1f}% (catches {sensitivity*100:.1f}% of deteriorating patients)\")\n",
        "print(f\"  Specificity: {specificity*100:.1f}% (correctly identifies {specificity*100:.1f}% of stable patients)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clinical-interpretation"
      },
      "source": [
        "### üí° Clinical Interpretation\n",
        "\n",
        "**Question:** In a clinical early warning system, would you prefer:\n",
        "- A) Higher sensitivity (catch more deteriorating patients, but more false alarms)\n",
        "- B) Higher specificity (fewer false alarms, but might miss some deteriorations)\n",
        "\n",
        "There's no universally \"right\" answer - it depends on the clinical context!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part5-intro"
      },
      "source": [
        "## Part 5: Experimenting with Model Parameters\n",
        "\n",
        "ML models have settings called **hyperparameters** that control how they learn.\n",
        "\n",
        "For decision trees, one important parameter is `max_depth` - how many questions deep the tree can go.\n",
        "\n",
        "Let's see what happens when we change it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "experiment-depth"
      },
      "outputs": [],
      "source": [
        "# Compare different tree depths\n",
        "\n",
        "depths = [1, 2, 3, 4, 5, 10, 20, None]  # None = no limit\n",
        "results = []\n",
        "\n",
        "for depth in depths:\n",
        "    # Train model\n",
        "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on BOTH training and test data\n",
        "    train_acc = model.score(X_train, y_train)\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "\n",
        "    depth_str = str(depth) if depth else 'Unlimited'\n",
        "    results.append({\n",
        "        'depth': depth_str,\n",
        "        'train_accuracy': train_acc,\n",
        "        'test_accuracy': test_acc\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "x_pos = range(len(depths))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar([x - width/2 for x in x_pos], results_df['train_accuracy']*100,\n",
        "               width, label='Training Accuracy', color='steelblue')\n",
        "bars2 = ax.bar([x + width/2 for x in x_pos], results_df['test_accuracy']*100,\n",
        "               width, label='Test Accuracy', color='coral')\n",
        "\n",
        "ax.set_xlabel('Maximum Tree Depth', fontsize=12)\n",
        "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax.set_title('Effect of Tree Depth on Model Performance', fontsize=14)\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(results_df['depth'])\n",
        "ax.legend()\n",
        "ax.set_ylim(50, 105)\n",
        "ax.axhline(y=100, color='gray', linestyle=':', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overfitting-explanation"
      },
      "source": [
        "### üí° Understanding Overfitting\n",
        "\n",
        "Look at the graph above. Notice:\n",
        "\n",
        "1. **Training accuracy** keeps increasing as trees get deeper\n",
        "2. **Test accuracy** increases at first, then may plateau or decrease\n",
        "\n",
        "When training accuracy is MUCH higher than test accuracy, the model is **overfitting** - it memorised the training data instead of learning generalisable patterns.\n",
        "\n",
        "**The goal is to find the \"sweet spot\"** where the model is complex enough to capture real patterns, but not so complex that it memorises noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part6-intro"
      },
      "source": [
        "## Part 6: The Effect of Training Data Size\n",
        "\n",
        "ML models learn from data. What happens if we have less data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-size-experiment"
      },
      "outputs": [],
      "source": [
        "# Train with different amounts of data\n",
        "\n",
        "training_sizes = [50, 100, 200, 400, 600, 800]\n",
        "size_results = []\n",
        "\n",
        "for size in training_sizes:\n",
        "    # Take a subset of training data\n",
        "    X_subset = X_train[:size]\n",
        "    y_subset = y_train[:size]\n",
        "\n",
        "    # Train model\n",
        "    model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "    model.fit(X_subset, y_subset)\n",
        "\n",
        "    # Evaluate on full test set\n",
        "    test_acc = model.score(X_test, y_test)\n",
        "\n",
        "    size_results.append({\n",
        "        'training_size': size,\n",
        "        'test_accuracy': test_acc\n",
        "    })\n",
        "\n",
        "size_df = pd.DataFrame(size_results)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(size_df['training_size'], size_df['test_accuracy']*100, 'b-o', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Training Patients', fontsize=12)\n",
        "plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
        "plt.title('How Training Data Size Affects Model Performance', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìà Observations:\")\n",
        "print(\"  - More training data generally = better performance\")\n",
        "print(\"  - But returns diminish as you add more data\")\n",
        "print(\"  - This is why healthcare AI needs large datasets!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part7-intro"
      },
      "source": [
        "## Part 7: Feature Importance\n",
        "\n",
        "Which vital signs does the model think are most important for predicting deterioration?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature-importance"
      },
      "outputs": [],
      "source": [
        "# Train final model with good parameters\n",
        "final_model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = final_model.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=True)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
        "plt.xlabel('Importance Score', fontsize=12)\n",
        "plt.title('Which Vital Signs Matter Most for Predicting Deterioration?', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for _, row in importance_df.sort_values('Importance', ascending=False).iterrows():\n",
        "    print(f\"  {row['Feature']}: {row['Importance']:.3f}\")\n",
        "\n",
        "print(\"\\nüí° Higher importance = the model relies more on this feature\")\n",
        "print(\"   Does this match clinical intuition about deterioration?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part8-intro"
      },
      "source": [
        "## Part 8: üîß Hands-On Experimentation\n",
        "\n",
        "Now it's your turn! Use the cell below to experiment with different settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "your-experiment"
      },
      "outputs": [],
      "source": [
        "# ===== YOUR EXPERIMENT =====\n",
        "# Modify these parameters and see what happens!\n",
        "\n",
        "MY_TREE_DEPTH = 4           # Try: 1, 2, 3, 5, 10, None\n",
        "MY_MIN_SAMPLES = 5          # Minimum patients needed to make a split (try: 2, 5, 10, 20)\n",
        "\n",
        "# ============================\n",
        "\n",
        "# Train YOUR model\n",
        "my_model = DecisionTreeClassifier(\n",
        "    max_depth=MY_TREE_DEPTH,\n",
        "    min_samples_split=MY_MIN_SAMPLES,\n",
        "    random_state=42\n",
        ")\n",
        "my_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "train_acc = my_model.score(X_train, y_train)\n",
        "test_acc = my_model.score(X_test, y_test)\n",
        "\n",
        "# Predictions for confusion matrix\n",
        "my_predictions = my_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, my_predictions)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print(\"YOUR MODEL RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Settings: max_depth={MY_TREE_DEPTH}, min_samples={MY_MIN_SAMPLES}\")\n",
        "print(f\"\\nTraining Accuracy: {train_acc*100:.1f}%\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.1f}%\")\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  True Negatives: {tn}\")\n",
        "print(f\"  True Positives: {tp}\")\n",
        "print(f\"  False Negatives: {fn} (missed deteriorations)\")\n",
        "print(f\"  False Positives: {fp} (false alarms)\")\n",
        "print(f\"\\nSensitivity: {tp/(tp+fn)*100:.1f}%\")\n",
        "print(f\"Specificity: {tn/(tn+fp)*100:.1f}%\")\n",
        "\n",
        "# Check for overfitting\n",
        "if train_acc - test_acc > 0.1:\n",
        "    print(f\"\\n‚ö†Ô∏è  Warning: Model may be overfitting!\")\n",
        "    print(f\"    Training acc is {(train_acc-test_acc)*100:.1f}% higher than test acc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection-intro"
      },
      "source": [
        "## Part 9: Reflection Questions\n",
        "\n",
        "Consider these questions and write your thoughts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reflections"
      },
      "outputs": [],
      "source": [
        "# ===== YOUR REFLECTIONS =====\n",
        "\n",
        "reflections = \"\"\"\n",
        "1. Why is it important to test on data the model hasn't seen?\n",
        "   Your answer:\n",
        "\n",
        "\n",
        "2. What happens when a model is too complex (overfitting)?\n",
        "   Your answer:\n",
        "\n",
        "\n",
        "3. Why might a model perform differently on data from another hospital?\n",
        "   Your answer:\n",
        "\n",
        "\n",
        "4. The decision tree shows its \"reasoning\". Why might this be important\n",
        "   for healthcare AI?\n",
        "   Your answer:\n",
        "\n",
        "\n",
        "5. If you were deploying this model, what test accuracy would you require?\n",
        "   What sensitivity/specificity trade-off would you choose?\n",
        "   Your answer:\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(reflections)\n",
        "print(\"\\n‚úÖ Reflection saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deliverable"
      },
      "source": [
        "## üìù Deliverable\n",
        "\n",
        "Complete the guided notebook, including:\n",
        "1. Running all experiments\n",
        "2. Your own experimentation with parameters\n",
        "3. Written reflections\n",
        "\n",
        "Submit via LMS by the Week 2 deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üèÅ Summary\n",
        "\n",
        "In this exercise, you learned:\n",
        "\n",
        "‚úÖ **Training data** is used to teach the model patterns\n",
        "\n",
        "‚úÖ **Test data** (never seen by model) is used to evaluate real performance\n",
        "\n",
        "‚úÖ **Decision trees** make predictions through a series of questions\n",
        "\n",
        "‚úÖ **Overfitting** occurs when models memorise rather than generalise\n",
        "\n",
        "‚úÖ **More data** generally improves performance\n",
        "\n",
        "‚úÖ **Feature importance** shows which inputs the model relies on most\n",
        "\n",
        "**Key insight:** ML models learn patterns from data - they can only be as good as the data they're trained on!\n",
        "\n",
        "---\n",
        "\n",
        "**Next week:** We'll explore the data itself - where it comes from, what biases it might contain, and why that matters for healthcare AI."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
